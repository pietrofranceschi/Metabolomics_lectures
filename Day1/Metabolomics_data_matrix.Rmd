---
title: "Metabolomics Data Matrix"
author: "Pietro Franceschi"
output: 
  ioslides_presentation:
    css: "../mycss.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(knitr)
```



## Many variables in many samples

As in all _omic_ experiments, a metabolomic assay allows to measure a large number of _variables_(properties) in a (hopefully) reasonably large number of samples.

<br>
<hr>
<br>

~~... what are the variables ...~~

> 1. In a targeted metabolomics investigation?
> 2. In an untargeted MS based metabolomics analysis? 
> 3. In an untargeted NMR based metabolomics assay?

	
## Number of variables | place your guess {.build}

> * Targeted metabolomics investigation
> * ~~from 10 to 300~~
> * Unargeted metabolomics investigation
> * ~~from 1000 to 30000~~ 


## Data Matrix

```{r, echo=FALSE, fig.align='center', out.width="60%"}
include_graphics("../images/Matrix.svg")
```

## Variables are not independent {.smaller}

* Biological relations
* Chemical/Analytical reasons

```{r fig.align='center', out.width="100%"}
x <- 1:20
y <- x*2 + rnorm(20,0,3)
par(pty="s")
plot(x,y, main = "Correlation", xlab = "Var1", ylab = "Var2", cex = 1, col = "coral", pch = 19)
```

---



```{r, echo=FALSE, fig.align='center', out.width="50%"}
include_graphics("../images/important.png")
```

<br>

<div style="background-color:#73AD21;padding:20px;border-radius: 15px;color:black;text-align: center;">
  <p>Analytical correlation are always stronger than the biological one!</p>
</div>

<br>

*The correlation structure of your matrix is often not really informative about biology*


## Rows (samples) are often dependent

The design of the study can result in a ~~multilevel hierarchical structure~~  of the samples which violate independence. In other words some samples are "by construction" associated and share something in common

~~**Examples**~~

1. Repeated measures of the same individual
2. Subpopulations
3. Different site, different day, ...

## Data Matrix size

In the typical "happy" statistical context, the number of variable we measure is smaller than the number of samples. 

In _omics_ the number of variables normally outperforms the number of samples
<br>
<br>
~~*FAT DATA MATRIX*~~
```{r fig.align='center', fig.height=4, fig.width=12, out.width="100%"}
mymat <- matrix(rnorm(20*300), nrow = 20)
image(t(mymat), asp = 20/300, xlab = "", ylab = "", axes = FALSE)
# Add closer x label
mtext("Variables", side = 1, line = -3)  # smaller line = closer

# Add closer y label
mtext("Samples", side = 2, line = 0)
```

## Univariate approach

The **Univariate** approach considers each variable separately and it applies "standard" statistical tools to spot the more **interesting** variables

1. Statistical testing 
2. Linear modeling (`lm`, `glm`, ...)
3. ANOVAs
4. Hierarchical Modeling

```{r fig.align='center', fig.height=2.5, fig.width=6, out.width="60%"}
mymat <- matrix(rnorm(20*100), nrow = 20)
par(mar=rep(0, 4), xpd = NA) 
image(t(mymat), asp = 20/100, xaxt='n', ann=FALSE, yaxt='n', bty="n")
rect(xleft = 0.5, xright = 0.51, ybottom = -0.1, ytop = 1.1, border = "steelblue", lwd = 2)
rect(xleft = 0.3, xright = 0.31, ybottom = -0.1, ytop = 1.1, border = "steelblue", lwd = 2)
rect(xleft = 0.7, xright = 0.71, ybottom = -0.1, ytop = 1.1, border = "steelblue", lwd = 2)
```

## Multiple testing

* We measure many variables (features,metabolites) in the same set of samples
* We run a battery of statistical tests looking for the significance of what we see in the individual variables 
* **We ask ourselves if at least one variable is significant in the overall set of variables**

<br>
<hr>

~~We run individual tests, but we have question about the full set of variables ...~~



## Multivariate approach

Each samples is represented as a point in the multidimesional variable space. The dataset is a **cloud** of points in that space.

The size of the space equals the number of variables we are measuring. **Multivariate methods** (PCA, PLS, ASCA, ...) are able to exploit the correlation between the variables to highlight/extract the organization of the data

```{r, echo=FALSE, fig.align='center', out.width="40%"}
include_graphics("../images/Coord_planes_color.svg")
```

## Why multivariate

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6, out.width="50%"}
library(tidyverse)

# Function to generate points on an ellipse
generate_ellipse <- function(center = c(0, 0), axes = c(1, 0.5), angle = 0, n_points = 100) {
  theta <- seq(0, 2 * pi, length.out = n_points)
  # Parametric ellipse before rotation
  x <- axes[1] * cos(theta)
  y <- axes[2] * sin(theta)
  # Rotation matrix
  rot <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), nrow = 2)
  rotated <- t(rot %*% rbind(x, y))
  # Translate to center
  data.frame(
    x = rotated[, 1] + center[1],
    y = rotated[, 2] + center[2]
  )
}

# Create two ellipses
ellipse1 <- generate_ellipse(center = c(-1.5, 0), axes = c(2, 1), angle = pi / 6)
ellipse1$group <- "Ellipse 1"

ellipse2 <- generate_ellipse(center = c(1.5, 1), axes = c(2, 1), angle = pi / 6)
ellipse2$group <- "Ellipse 2"

# Combine into one data frame
ellipses <- rbind(ellipse1, ellipse2)

# Plot
ggplot(ellipses, aes(x = x, y = y, fill = group, col = group)) +
  geom_path(size = 1.2) +
  geom_polygon(alpha = 0.7) + 
  geom_text(aes(x = -1.5, y = 0, label = "A"), col = "black") + 
  geom_text(aes(x = 1.5, y = 1, label = "B"), col = "black") + 
  xlab("Var 1") + 
  ylab("Var 2") + 
  coord_equal() +
  theme_bw() +
  theme(legend.position = "none", aspect.ratio = 1)
```

Group separation is clearer in the 2d space ...

## Multivariate


~~**PRO**~~ ðŸ˜€

1. Potentially more powerful
2. Explicit use of variable correlation
3. No issues with multiple testing

<hr>

~~**CONS**~~ ðŸ˜ž


1. Chance correlations in fat matrices
2. Empty Space
3. Difficult to embed hierarchical structure

## Univariate


~~**PRO**~~ ðŸ˜€


1. Statistical modeling is there!
2. Interpretable by construction

<hr>

~~**CONS**~~ ðŸ˜ž


1. Multiple testing
2. The structure of the data creates redundancy
3. Assumptions for parametric (and non parametric) approaches are often not fulfilled


## The course of dimensionality

```{r fig.align='center', fig.height=2.5, fig.width=8, out.width="100%"}
set.seed(123)

par(mfrow = c(1,3))
par(pty="s")
plot(y = rep(0,10), x = runif(10,0,10), xaxt='n', yaxt='n', bty="n", xlim = c(-1,11), 
     main = "10 points 1D", xlab = "", ylab = "", col = "#FFA50060", cex = 2, pch  =19)
arrows(x0 = 0 , y0 = 0, x1 = 11)

par(pty="s")
plot(y = runif(10,0,10), x = runif(10,0,10), xlab = "D1", ylab = "D2", main = "10 points 2D", col = "#FFA50060", cex = 2, pch =19)


plot(y = runif(100,0,10), x = runif(100,0,10), xlab = "D1", ylab = "D2", main = "100 points 2D", col = "#FFA50060", cex = 2, pch  =19)

```

1. To fill the space the number of points is not linear with the number of dimensions
2. Already with 10 samples the 2d plot looks empty
3. Can you imagine 20 samples in 10000 dimensions? ;-)

 






