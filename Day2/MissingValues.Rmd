---
title: "Missing Values"
author: "Pietro Franceschi"
output: 
  ioslides_presentation:
    css: "../mycss.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(tidyverse)
```

## Missing Values

_In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data._

<br>

- Data can be missing **at random** or **not at random**
- In **metabolomics** missing values arise when, for a sample, it was not possible to get a specific intensity for a feature/metabolite


## Notes {#notes}

In metabolomics missing values pop up for different reasons

1. There was an error during preprocessing (targeted or untargeted)
2. For a sample the concentration of a metabolite was really low
3. A metabolite was "absent" in a sample I'll get a missing value (_eg. red pigment in a an extract of white grape_)

<br>

1,2 & 3 are **random** or **not random** ?




## Analytical Absence {#highlights}

```{r, echo=FALSE, fig.align='center', out.width="50%"}
include_graphics("../images/important.png")
```

<br>

Having a **Missing** is **not** equivalent of having **zero** concentration!

- Each analytical method is characterized by a well defined *detection limit* 
- Every concentration between zero and the detection limit will not be detected
- We never can be 100% sure that something is absent from my sample ...

Where can we get the detection limit ?



## Scenario 1: 1 class

Signal close to the detection limit

```{r fig.align='center', out.width="90%"}
set.seed(123)
sc1 <- rnorm(50,20,2)

col1 <- rep("#c62d4280",50)
col1[sc1 < 19] <- "#377eb880"

plot(sc1, pch = 19, col = col1, ylab = "Signal", xlab = "Samples", xaxt = "n", ylim = c(10,30))
abline(h = 19, col = "#4daf4a", lty = 2, lwd =2)
text(x = 50, y = 20, labels = "Detection Limit", pos = 2)

```

Missing values are "randomly" popping-up


## Scenario 2: 2 class biomarker

```{r fig.align='center', out.width="90%"}
set.seed(123)
sc2 <- c(rnorm(25,20,2), rnorm(25,15,2))

col2 <- rep("#c62d4280",50)
col2[sc2 < 19] <- "#377eb880"

plot(sc2, pch = c(rep(19,25),rep(4,25)), col = col2, ylab = "Signal", xlab = "Samples", xaxt = "n", ylim = c(10,30))
abline(h = 19, col = "#4daf4a", lty = 2, lwd =2)
text(x = 50, y = 20, labels = "Detection Limit", pos = 2)
legend("topright", legend = c("Class A", "Class B"), pch = c(19,3), bty = "n")

```

Missing values are ~~not~~ "randomly" popping-up


## Scenario 3: 2 class ??

```{r fig.align='center', out.width="90%"}
set.seed(123)
sc3 <- c(25,rnorm(24,15,2), rnorm(25,10,2))

col3 <- rep("#c62d4280",50)
col3[sc3 < 19] <- "#377eb880"

plot(sc3, pch = c(rep(19,25),rep(3,25)), col = col3, ylab = "Signal", xlab = "Samples",  xaxt = "n", ylim = c(7,30))
abline(h = 19, col = "#4daf4a", lty = 2, lwd =2)
text(x = 50, y = 20, labels = "Detection Limit", pos = 2)
legend("topright", legend = c("Class A", "Class B"), pch = c(19,3), bty = "n")

```

What would you do here?

## Dealing with missing values

- Use statistical methods able to handle missing data
- ~~Impute~~ them put a reasonable number with variability
- Remove features with too many NAs (scenario 3)

<br>
<hr>


How many NAs are acceptable **depends on the allocation of the samples to the factors of the study** 


## Imputation {#notes}

**Imputation** is the process of substituting a missing value with a reasonable number:

> 1. multivariate imputation uses the value of samples whicha are close in the multivariate space to select a good number (`missMDA` package, KNN imputation, ... ). This works well if data are missing at random ...
> 2. a reasonable number can be chosen on the bases of analytical considerations (e.g. a random number between zero and the detection limit)
> 3. the imputation strategy should be not "aware" of the design of the study

## Further Observations

* It is "easier" to handle missing values randomly distributed
* Use **domain specific knowledge** (e.g analytical - LOD) inject new knowledge in the data analysis pipeline!
* **Quality Check**: try different forms of imputation: are the outcomes sensitive to that?





